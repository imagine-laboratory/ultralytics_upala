{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ultralytics YOLO output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  Python>=3.10 is required, but Python==3.8.18 is currently installed \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'8.3.11'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ultralytics\n",
    "ultralytics.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set an environment variable\n",
    "os.environ['YOLO_VERBOSE'] = \"False\"\n",
    "\n",
    "# Access the environment variable\n",
    "print(os.environ['YOLO_VERBOSE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics_to_txt(metrics_dict, filename=\"metrics_results.txt\"):\n",
    "    \"\"\"\n",
    "    Save the given dictionary of metrics to a text file.\n",
    "    \n",
    "    Args:\n",
    "    - metrics_dict (dict): Dictionary containing metric names as keys and their values.\n",
    "    - filename (str): The name of the file to save the metrics. Default is \"metrics_results.txt\".\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        for metric, value in metrics_dict.items():\n",
    "            f.write(f\"{metric}: {value}\\n\")\n",
    "    \n",
    "    print(f\"Metrics saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO, RTDETR\n",
    "# Load the weights of the YOLO model\n",
    "model_name = \"yolo11x_DJI_0008_V_and_0010_V_2@fine-tuning\"\n",
    "\n",
    "model = YOLO(f\"C:/Users/dnnxl/Documents/GitHub/drone-sort/weights/{model_name}/best.pt\", verbose=False)\n",
    "#model = RTDETR(f\"C:/Users/dnnxl/Documents/GitHub/drone-sort/weights/{model_name}/best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = model([files_in_directory[160]])\n",
    "#xyxy_results = results[0].boxes.cpu().numpy()\n",
    "#xyxy_results.xyxy, xyxy_results.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "# Concatenate scores as the fifth column\n",
    "#result = np.concatenate([xyxy_results.xyxy, xyxy_results.conf.reshape(-1, 1)], axis=1)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.11  Python-3.8.18 torch-2.1.0 CPU (12th Gen Intel Core(TM) i5-1235U)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11x summary (fused): 464 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\dnnxl\\Documents\\GitHub\\drone-sort\\dataset\\DJI_20240308111117_0010_V_1\\valid\\labels.cache... 329 images, 160 backgrounds, 0 corrupt: 100%|██████████| 489/489 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [17:14<00:00, 33.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        489       1891      0.965      0.883      0.951      0.765\n",
      "Speed: 5.7ms preprocess, 1980.5ms inference, 0.0ms loss, 2.9ms postprocess per image\n",
      "Saving runs\\detect\\yolo11x_DJI_0008_V_and_0010_V_2@fine-tuning@DJI_20240308111117_0010_V_1\\predictions.json...\n",
      "Results saved to \u001b[1mruns\\detect\\yolo11x_DJI_0008_V_and_0010_V_2@fine-tuning@DJI_20240308111117_0010_V_1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "video_name = \"DJI_20240308111117_0010_V_1\"\n",
    "run_name = model_name + \"@\" + video_name\n",
    "\n",
    "data_yaml_file = f\"C:/Users/dnnxl/Documents/GitHub/drone-sort/dataset/{video_name}/data.yaml\"\n",
    "#data_yaml_file = \"C:/Users/dnnxl/Documents/GitHub/drone-sort/dataset/DJI_20240308111117_0010_V_2/data.yaml\"\n",
    "#data_yaml_file = \"C:/Users/dnnxl/Documents/GitHub/drone-sort/dataset/DJI_20240308110958_0008_V_1/data.yaml\"\n",
    "#data_yaml_file = \"C:/Users/dnnxl/Documents/GitHub/drone-sort/dataset/DJI_20240308110958_0008_V_2/data.yaml\"\n",
    "\n",
    "validation_results = model.val(\n",
    "    data=data_yaml_file, imgsz=640, batch=16, conf=0.25, iou=0.6, \n",
    "    device=\"cpu\", save_json=True, name=f\"{run_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.11  Python-3.8.18 torch-2.1.0 CPU (12th Gen Intel Core(TM) i5-1235U)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\dnnxl\\Documents\\GitHub\\drone-sort\\dataset\\DJI_20240308110228_0006_V_2\\valid\\labels.cache... 387 images, 12 backgrounds, 0 corrupt: 100%|██████████| 399/399 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [12:04<00:00, 28.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        399       5692      0.925      0.758      0.862      0.621\n",
      "Speed: 4.3ms preprocess, 1692.9ms inference, 0.0ms loss, 3.0ms postprocess per image\n",
      "Saving runs\\detect\\yolo11x_DJI_0008_V_and_0010_V_2@fine-tuning@DJI_20240308110228_0006_V_2\\predictions.json...\n",
      "Results saved to \u001b[1mruns\\detect\\yolo11x_DJI_0008_V_and_0010_V_2@fine-tuning@DJI_20240308110228_0006_V_2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "video_name = \"DJI_20240308110228_0006_V_2\"\n",
    "run_name = model_name + \"@\" + video_name\n",
    "\n",
    "data_yaml_file = f\"C:/Users/dnnxl/Documents/GitHub/drone-sort/dataset/{video_name}/data.yaml\"\n",
    "#data_yaml_file = \"C:/Users/dnnxl/Documents/GitHub/drone-sort/dataset/DJI_20240308111117_0010_V_2/data.yaml\"\n",
    "#data_yaml_file = \"C:/Users/dnnxl/Documents/GitHub/drone-sort/dataset/DJI_20240308110958_0008_V_1/data.yaml\"\n",
    "#data_yaml_file = \"C:/Users/dnnxl/Documents/GitHub/drone-sort/dataset/DJI_20240308110958_0008_V_2/data.yaml\"\n",
    "\n",
    "validation_results = model.val(\n",
    "    data=data_yaml_file, imgsz=640, batch=16, conf=0.25, iou=0.6, \n",
    "    device=\"cpu\", save_json=True, name=f\"{run_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.11  Python-3.8.18 torch-2.1.0 CPU (12th Gen Intel Core(TM) i5-1235U)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\dnnxl\\Documents\\GitHub\\drone-sort\\dataset\\DJI_20240308110115_0005_V\\valid\\labels.cache... 440 images, 34 backgrounds, 0 corrupt: 100%|██████████| 474/474 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 30/30 [11:01<00:00, 22.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        474       4997      0.923      0.897       0.94      0.719\n",
      "Speed: 2.9ms preprocess, 1306.5ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Saving runs\\detect\\yolo11x_DJI_0008_V_and_0010_V_2@fine-tuning@DJI_20240308110115_0005_V\\predictions.json...\n",
      "Results saved to \u001b[1mruns\\detect\\yolo11x_DJI_0008_V_and_0010_V_2@fine-tuning@DJI_20240308110115_0005_V\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "video_name = \"DJI_20240308110115_0005_V\"\n",
    "run_name = model_name + \"@\" + video_name\n",
    "\n",
    "data_yaml_file = f\"C:/Users/dnnxl/Documents/GitHub/drone-sort/dataset/{video_name}/data.yaml\"\n",
    "#data_yaml_file = \"C:/Users/dnnxl/Documents/GitHub/drone-sort/dataset/DJI_20240308111117_0010_V_2/data.yaml\"\n",
    "#data_yaml_file = \"C:/Users/dnnxl/Documents/GitHub/drone-sort/dataset/DJI_20240308110958_0008_V_1/data.yaml\"\n",
    "#data_yaml_file = \"C:/Users/dnnxl/Documents/GitHub/drone-sort/dataset/DJI_20240308110958_0008_V_2/data.yaml\"\n",
    "\n",
    "validation_results = model.val(\n",
    "    data=data_yaml_file, imgsz=640, batch=16, conf=0.25, iou=0.6, \n",
    "    device=\"cpu\", save_json=True, name=f\"{run_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.11  Python-3.8.18 torch-2.1.0 CPU (12th Gen Intel Core(TM) i5-1235U)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\dnnxl\\Documents\\GitHub\\drone-sort\\dataset\\DJI_20240308110013_0004_V_1\\valid\\labels.cache... 584 images, 5 backgrounds, 0 corrupt: 100%|██████████| 589/589 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 37/37 [16:28<00:00, 26.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        589      11124      0.893      0.822      0.889       0.62\n",
      "Speed: 3.9ms preprocess, 1551.5ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "Saving runs\\detect\\yolo11x_DJI_0008_V_and_0010_V_2@fine-tuning@DJI_20240308110013_0004_V_1\\predictions.json...\n",
      "Results saved to \u001b[1mruns\\detect\\yolo11x_DJI_0008_V_and_0010_V_2@fine-tuning@DJI_20240308110013_0004_V_1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "video_name = \"DJI_20240308110013_0004_V_1\"\n",
    "run_name = model_name + \"@\" + video_name\n",
    "\n",
    "data_yaml_file = f\"C:/Users/dnnxl/Documents/GitHub/drone-sort/dataset/{video_name}/data.yaml\"\n",
    "#data_yaml_file = \"C:/Users/dnnxl/Documents/GitHub/drone-sort/dataset/DJI_20240308111117_0010_V_2/data.yaml\"\n",
    "#data_yaml_file = \"C:/Users/dnnxl/Documents/GitHub/drone-sort/dataset/DJI_20240308110958_0008_V_1/data.yaml\"\n",
    "#data_yaml_file = \"C:/Users/dnnxl/Documents/GitHub/drone-sort/dataset/DJI_20240308110958_0008_V_2/data.yaml\"\n",
    "\n",
    "validation_results = model.val(\n",
    "    data=data_yaml_file, imgsz=640, batch=16, conf=0.25, iou=0.6, \n",
    "    device=\"cpu\", save_json=True, name=f\"{run_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.11  Python-3.8.18 torch-2.1.0 CPU (12th Gen Intel Core(TM) i5-1235U)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\dnnxl\\Documents\\GitHub\\drone-sort\\dataset\\DJI_20240308110013_0004_V_3\\valid\\labels.cache... 253 images, 27 backgrounds, 0 corrupt: 100%|██████████| 280/280 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [08:14<00:00, 27.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        280       6209      0.882      0.811      0.879       0.61\n",
      "Speed: 5.1ms preprocess, 1611.4ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Saving runs\\detect\\yolo11x_DJI_0008_V_and_0010_V_2@fine-tuning@DJI_20240308110013_0004_V_3\\predictions.json...\n",
      "Results saved to \u001b[1mruns\\detect\\yolo11x_DJI_0008_V_and_0010_V_2@fine-tuning@DJI_20240308110013_0004_V_3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "video_name = \"DJI_20240308110013_0004_V_3\"\n",
    "run_name = model_name + \"@\" + video_name\n",
    "\n",
    "data_yaml_file = f\"C:/Users/dnnxl/Documents/GitHub/drone-sort/dataset/{video_name}/data.yaml\"\n",
    "#data_yaml_file = \"C:/Users/dnnxl/Documents/GitHub/drone-sort/dataset/DJI_20240308111117_0010_V_2/data.yaml\"\n",
    "#data_yaml_file = \"C:/Users/dnnxl/Documents/GitHub/drone-sort/dataset/DJI_20240308110958_0008_V_1/data.yaml\"\n",
    "#data_yaml_file = \"C:/Users/dnnxl/Documents/GitHub/drone-sort/dataset/DJI_20240308110958_0008_V_2/data.yaml\"\n",
    "\n",
    "validation_results = model.val(\n",
    "    data=data_yaml_file, imgsz=640, batch=16, conf=0.25, iou=0.6, \n",
    "    device=\"cpu\", save_json=True, name=f\"{run_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to ./runs\\detect\\yolo11x_DJI_0008_V_and_0010_V_2@fine-tuning@DJI_20240308110013_0004_V_3/output.txt\n"
     ]
    }
   ],
   "source": [
    "save_metrics_to_txt(validation_results.results_dict, filename=\"./\"+ str(validation_results.save_dir) + \"/output.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference/Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_files(directory):\n",
    "    try:\n",
    "        # List all files in the given directory\n",
    "        files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "        return files\n",
    "    except Exception as e:\n",
    "        return str(e)  # Return the error message as a string\n",
    "\n",
    "def list_directories(path='.'):\n",
    "    return [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "\n",
    "# Example usage:\n",
    "directory_path = f\"C:/Users/dnnxl/Documents/GitHub/drone-sort/dataset/{video_name}/valid/images\"\n",
    "#directory_path = \"C:/Users/dnnxl/Documents/GitHub/drone-sort/dataset/DJI_20240308111117_0010_V_2/valid/images\"\n",
    "#directory_path = \"C:/Users/dnnxl/Documents/GitHub/drone-sort/dataset/DJI_20240308110958_0008_V_1/valid/images\"\n",
    "#directory_path = \"C:/Users/dnnxl/Documents/GitHub/drone-sort/dataset/DJI_20240308110958_0008_V_2/valid/images\"\n",
    "\n",
    "files_in_directory = [os.path.join(directory_path, file) for file in list_files(directory_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert into MOT format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def process_mot20_results(files_in_directory, model, output_file=\"output_mot.txt\"):\n",
    "    mot_results = []\n",
    "\n",
    "    # Use tqdm to display a progress bar while iterating over frames\n",
    "    for frame in tqdm(files_in_directory, desc=\"Processing frames\"):\n",
    "        results = model([frame])  # Run model on the frame\n",
    "\n",
    "        # Extract the filename and frame number\n",
    "        filename = os.path.basename(frame)\n",
    "        frame_number = int(filename.split('_')[1].split('.')[0])\n",
    "\n",
    "        # Process each result from the model output\n",
    "        for result in results:\n",
    "            boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "            box_data = boxes.xywh  # Get [x_center, y_center, width, height] format\n",
    "            conf = boxes.conf  # Confidence scores\n",
    "            cls = boxes.cls  # Class labels (if needed for IDs)\n",
    "\n",
    "            # Loop through each detected box\n",
    "            for i, (xywh, score) in enumerate(zip(box_data, conf)):\n",
    "                bb_left = xywh[0] - (xywh[2] / 2)  # Convert x_center to bb_left\n",
    "                bb_top = xywh[1] - (xywh[3] / 2)   # Convert y_center to bb_top\n",
    "                bb_width = xywh[2]\n",
    "                bb_height = xywh[3]\n",
    "                obj_id = -1  # Assign a unique ID (or use a tracker to get actual IDs)\n",
    "\n",
    "                class_id = 1\n",
    "                # Format: frame, id, bb_left, bb_top, bb_width, bb_height, conf, -1, -1, -1\n",
    "                # MOT15 frame_number, identity_number, bbox left, bbox_top, bbox_width, bbox_height, confidence, class, visibility\n",
    "                mot_line = f\"{frame_number}, {obj_id}, {bb_left:.2f}, {bb_top:.2f}, {bb_width:.2f}, {bb_height:.2f}, {score:.2f}, {class_id}, 1\"\n",
    "                # MOT15 frame_number, identity_number, bbox left, bbox_top, bbox_width, bbox_height, confidence, x, y, z\n",
    "                # mot_line = f\"{frame_number}, {obj_id}, {bb_left:.2f}, {bb_top:.2f}, {bb_width:.2f}, {bb_height:.2f}, {score:.2f}, -1, -1, -1\"\n",
    "\n",
    "                mot_results.append(mot_line)\n",
    "\n",
    "    # Write results to a .txt file in MOT format\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"\\n\".join(mot_results))\n",
    "    print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_mot20_results(files_in_directory, model, output_file=\"output_mot.txt\"):\n",
    "    mot_results = []\n",
    "\n",
    "    # Use tqdm to display a progress bar while iterating over frames\n",
    "    for frame in tqdm(files_in_directory, desc=\"Processing frames\"):\n",
    "        results = model([frame])  # Run model on the frame\n",
    "\n",
    "        # Extract the filename and frame number\n",
    "        filename = os.path.basename(frame)\n",
    "        frame_number = int(filename.split('_')[1].split('.')[0])\n",
    "\n",
    "        # Process each result from the model output\n",
    "        for result in results:\n",
    "            boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "            box_data = boxes.xyxy  # Get [x_center, y_center, width, height] format\n",
    "            conf = boxes.conf  # Confidence scores\n",
    "            cls = boxes.cls  # Class labels (if needed for IDs)\n",
    "\n",
    "            # Loop through each detected box\n",
    "            for i, (xyxy, score) in enumerate(zip(box_data, conf)):\n",
    "                bb_left = xyxy[0]  # Convert x_center to bb_left\n",
    "                bb_top = xyxy[1]  # Convert y_center to bb_top\n",
    "                bb_width = xyxy[2]-xyxy[0]\n",
    "                bb_height = xyxy[3]-xyxy[1]\n",
    "                obj_id = -1  # Assign a unique ID (or use a tracker to get actual IDs)\n",
    "\n",
    "                class_id = 1\n",
    "                # Format: frame, id, bb_left, bb_top, bb_width, bb_height, conf, -1, -1, -1\n",
    "                # MOT15 frame_number, identity_number, bbox left, bbox_top, bbox_width, bbox_height, confidence, class, visibility\n",
    "                mot_line = f\"{frame_number}, {obj_id}, {bb_left:.2f}, {bb_top:.2f}, {bb_width:.2f}, {bb_height:.2f}, {score:.2f}, {class_id}, 1\"\n",
    "                # MOT15 frame_number, identity_number, bbox left, bbox_top, bbox_width, bbox_height, confidence, x, y, z\n",
    "                # mot_line = f\"{frame_number}, {obj_id}, {bb_left:.2f}, {bb_top:.2f}, {bb_width:.2f}, {bb_height:.2f}, {score:.2f}, -1, -1, -1\"\n",
    "\n",
    "                mot_results.append(mot_line)\n",
    "\n",
    "    # Write results to a .txt file in MOT format\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"\\n\".join(mot_results))\n",
    "    print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "PATH_WEIGTHTS = 'C:/Users/dnnxl/Documents/GitHub/drone-sort/weights'\n",
    "directory_models = list_directories(PATH_WEIGTHTS)\n",
    "weights_in_directory = [os.path.join(PATH_WEIGTHTS, file, \"best.pt\") for file in list_directories(PATH_WEIGTHTS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO, RTDETR\n",
    "# Load the weights of the YOLO model\n",
    "\n",
    "OUTPATH_MOT_FILES = f'../output/mot_detections/{video_name}'\n",
    "\n",
    "# Create the directory\n",
    "try:\n",
    "    os.makedirs(OUTPATH_MOT_FILES, exist_ok=True)  # Creates intermediate directories if needed\n",
    "    print(f\"Directory created at: {OUTPATH_MOT_FILES}\")\n",
    "except OSError as e:\n",
    "    print(f\"Error creating directory: {e}\")\n",
    "\n",
    "for model_name, best_weight in zip(directory_models, weights_in_directory):\n",
    "    if model_name == 'rtdetrl':\n",
    "        model = RTDETR(best_weight, verbose=False)\n",
    "    else:\n",
    "        model = YOLO(best_weight, verbose=False)\n",
    "\n",
    "    output_mot_file = os.path.join(OUTPATH_MOT_FILES, f\"{model_name}.txt\")\n",
    "    process_mot20_results(files_in_directory, model, output_file=output_mot_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of output ultralytics results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_directory=files_in_directory[575:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "directory = f'./YOLO/pred/{video_name}'\n",
    "\n",
    "# Create the directory\n",
    "try:\n",
    "    os.makedirs(directory, exist_ok=True)  # Creates intermediate directories if needed\n",
    "    print(f\"Directory created at: {directory}\")\n",
    "except OSError as e:\n",
    "    print(f\"Error creating directory: {e}\")\n",
    "\n",
    "for file_name in files_in_directory:\n",
    "    results = model(file_name, iou=0.5, conf=0.3)\n",
    "    # Load the image\n",
    "    image = mpimg.imread(file_name)\n",
    "    \n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    # Process results for the current image\n",
    "    for result in results:\n",
    "        for bbox in result.boxes.xyxy:\n",
    "            # Extract bbox coordinates\n",
    "            xmin, ymin, xmax, ymax = bbox\n",
    "            width = xmax - xmin\n",
    "            height = ymax - ymin\n",
    "            \n",
    "            # Create a rectangle patch\n",
    "            rect = patches.Rectangle((xmin, ymin), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "            \n",
    "            # Add the rectangle patch to the plot\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    # Save the output image with bounding boxes\n",
    "    file_name_basename = os.path.basename(file_name)\n",
    "    output_file_name = file_name_basename\n",
    "    output_path = os.path.join(directory, output_file_name)\n",
    "    plt.axis('off')  # Hide axis\n",
    "    plt.savefig(output_path, bbox_inches='tight', pad_inches=0)  # Save without extra margins\n",
    "    \n",
    "    # Clear the current plot to prepare for the next image\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deteccion pero no exacto\n",
    "# Para una finca cuántas piñas tiene linea o cuadrante, atraviesa a pie y un empleado va contando \n",
    "# atraviesan 2% muestreo de cuánta piña si la precision es similar o si es similar a pie"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam-pinas-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
